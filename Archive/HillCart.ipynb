{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code Example: https://github.com/adventuresinML/adventures-in-ml-code/blob/master/r_learning_tensorflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import modules and initialize constants\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.0001\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def __init__(self, num_states, num_actions, batch_size):\n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._batch_size = batch_size\n",
    "        # define the placeholders\n",
    "        self._states = None\n",
    "        self._actions = None\n",
    "        # the output operations\n",
    "        self._logits = None\n",
    "        self._optimizer = None\n",
    "        self._var_init = None\n",
    "        # now setup the model\n",
    "        self._define_model()\n",
    "\n",
    "    def _define_model(self):\n",
    "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
    "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
    "        # create a couple of fully connected hidden layers\n",
    "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
    "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
    "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
    "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
    "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        self._var_init = tf.global_variables_initializer()\n",
    "\n",
    "    def predict_one(self, state, sess):\n",
    "        return sess.run(self._logits, feed_dict={self._states:\n",
    "                                                     state.reshape(1, self.num_states)})\n",
    "\n",
    "    def predict_batch(self, states, sess):\n",
    "        return sess.run(self._logits, feed_dict={self._states: states})\n",
    "\n",
    "    def train_batch(self, sess, x_batch, y_batch):\n",
    "        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})\n",
    "\n",
    "    @property\n",
    "    def num_states(self):\n",
    "        return self._num_states\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return self._num_actions\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def var_init(self):\n",
    "        return self._var_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GameRunner:\n",
    "    def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
    "                 decay, render=True):\n",
    "        self._sess = sess\n",
    "        self._env = env\n",
    "        self._model = model\n",
    "        self._memory = memory\n",
    "        self._render = render\n",
    "        self._max_eps = max_eps\n",
    "        self._min_eps = min_eps\n",
    "        self._decay = decay\n",
    "        self._eps = self._max_eps\n",
    "        self._steps = 0\n",
    "        self._reward_store = []\n",
    "        self._max_x_store = []\n",
    "\n",
    "    def run(self):\n",
    "        state = self._env.reset()\n",
    "        tot_reward = 0\n",
    "        max_x = -100\n",
    "        while True:\n",
    "            if self._render:\n",
    "                self._env.render()\n",
    "\n",
    "            action = self._choose_action(state)\n",
    "            next_state, reward, done, info = self._env.step(action)\n",
    "            if next_state[0] >= 0.1:\n",
    "                reward += 10\n",
    "            elif next_state[0] >= 0.25:\n",
    "                reward += 20\n",
    "            elif next_state[0] >= 0.5:\n",
    "                reward += 100\n",
    "\n",
    "            if next_state[0] > max_x:\n",
    "                max_x = next_state[0]\n",
    "            # is the game complete? If so, set the next state to\n",
    "            # None for storage sake\n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            self._memory.add_sample((state, action, reward, next_state))\n",
    "            self._replay()\n",
    "\n",
    "            # exponentially decay the eps value\n",
    "            self._steps += 1\n",
    "            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \\\n",
    "                                      * math.exp(-LAMBDA * self._steps)\n",
    "\n",
    "            # move the agent to the next state and accumulate the reward\n",
    "            state = next_state\n",
    "            tot_reward += reward\n",
    "\n",
    "            # if the game is done, break the loop\n",
    "            if done:\n",
    "                self._reward_store.append(tot_reward)\n",
    "                self._max_x_store.append(max_x)\n",
    "                break\n",
    "\n",
    "        print(\"Step {}, Total reward: {}, Eps: {}\".format(self._steps, tot_reward, self._eps))\n",
    "\n",
    "    def _choose_action(self, state):\n",
    "        if random.random() < self._eps:\n",
    "            return random.randint(0, self._model.num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self._model.predict_one(state, self._sess))\n",
    "\n",
    "    def _replay(self):\n",
    "        batch = self._memory.sample(self._model.batch_size)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        next_states = np.array([(np.zeros(self._model.num_states)\n",
    "                                 if val[3] is None else val[3]) for val in batch])\n",
    "        # predict Q(s,a) given the batch of states\n",
    "        q_s_a = self._model.predict_batch(states, self._sess)\n",
    "        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
    "        q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
    "        # setup training arrays\n",
    "        x = np.zeros((len(batch), self._model.num_states))\n",
    "        y = np.zeros((len(batch), self._model.num_actions))\n",
    "        for i, b in enumerate(batch):\n",
    "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
    "            # get the current q values for all actions in state\n",
    "            current_q = q_s_a[i]\n",
    "            # update the q value for action\n",
    "            if next_state is None:\n",
    "                # in this case, the game completed after action, so there is no max Q(s',a')\n",
    "                # prediction possible\n",
    "                current_q[action] = reward\n",
    "            else:\n",
    "                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
    "            x[i] = state\n",
    "            y[i] = current_q\n",
    "        self._model.train_batch(self._sess, x, y)\n",
    "\n",
    "    @property\n",
    "    def reward_store(self):\n",
    "        return self._reward_store\n",
    "\n",
    "    @property\n",
    "    def max_x_store(self):\n",
    "        return self._max_x_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "WARNING:tensorflow:From <ipython-input-2-5320e6ad77e2>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/anton/.virtualenvs/dl/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Episode 1 of 300\n",
      "Step 200, Total reward: -200.0, Eps: 0.9803966865736877\n",
      "Step 400, Total reward: -200.0, Eps: 0.9611815447608\n",
      "Step 600, Total reward: -200.0, Eps: 0.9423468882484062\n",
      "Step 800, Total reward: -200.0, Eps: 0.9238851829227694\n",
      "Step 1000, Total reward: -200.0, Eps: 0.9057890438555999\n",
      "Step 1200, Total reward: -200.0, Eps: 0.888051232349986\n",
      "Step 1400, Total reward: -200.0, Eps: 0.8706646530448178\n",
      "Step 1600, Total reward: -200.0, Eps: 0.8536223510765493\n",
      "Step 1800, Total reward: -200.0, Eps: 0.8369175092971592\n",
      "Step 2000, Total reward: -200.0, Eps: 0.820543445547202\n",
      "Episode 11 of 300\n",
      "Step 2200, Total reward: -200.0, Eps: 0.8044936099828537\n",
      "Step 2400, Total reward: -200.0, Eps: 0.7887615824558878\n",
      "Step 2600, Total reward: -200.0, Eps: 0.7733410699455306\n",
      "Step 2800, Total reward: -200.0, Eps: 0.7582259040411682\n",
      "Step 3000, Total reward: -200.0, Eps: 0.7434100384749007\n",
      "Step 3200, Total reward: -200.0, Eps: 0.7288875467029541\n",
      "Step 3400, Total reward: -200.0, Eps: 0.7146526195349836\n",
      "Step 3600, Total reward: -200.0, Eps: 0.7006995628103208\n",
      "Step 3800, Total reward: -200.0, Eps: 0.6870227951202322\n",
      "Step 4000, Total reward: -200.0, Eps: 0.6736168455752829\n",
      "Episode 21 of 300\n",
      "Step 4200, Total reward: -200.0, Eps: 0.6604763516169062\n",
      "Step 4400, Total reward: -200.0, Eps: 0.64759605687231\n",
      "Step 4600, Total reward: -200.0, Eps: 0.6349708090518567\n",
      "Step 4800, Total reward: -200.0, Eps: 0.6225955578880794\n",
      "Step 5000, Total reward: -200.0, Eps: 0.6104653531155071\n",
      "Step 5200, Total reward: -200.0, Eps: 0.5985753424904925\n",
      "Step 5400, Total reward: -200.0, Eps: 0.5869207698502498\n",
      "Step 5600, Total reward: -200.0, Eps: 0.5754969732103268\n",
      "Step 5800, Total reward: -200.0, Eps: 0.564299382899748\n",
      "Step 6000, Total reward: -200.0, Eps: 0.5533235197330861\n",
      "Episode 31 of 300\n",
      "Step 6200, Total reward: -200.0, Eps: 0.5425649932187278\n",
      "Step 6400, Total reward: -200.0, Eps: 0.5320194998026181\n",
      "Step 6600, Total reward: -200.0, Eps: 0.5216828211467822\n",
      "Step 6800, Total reward: -200.0, Eps: 0.5115508224419336\n",
      "Step 7000, Total reward: -100.0, Eps: 0.5016194507534953\n",
      "Step 7200, Total reward: -200.0, Eps: 0.4918847334003719\n",
      "Step 7400, Total reward: -200.0, Eps: 0.48234277636582407\n",
      "Step 7600, Total reward: -200.0, Eps: 0.47298976273981014\n",
      "Step 7800, Total reward: -200.0, Eps: 0.4638219511921713\n",
      "Step 8000, Total reward: -200.0, Eps: 0.45483567447604933\n",
      "Episode 41 of 300\n",
      "Step 8200, Total reward: -200.0, Eps: 0.44602733796093924\n",
      "Step 8400, Total reward: -200.0, Eps: 0.4373934181947889\n",
      "Step 8600, Total reward: -200.0, Eps: 0.4289304614945713\n",
      "Step 8800, Total reward: -200.0, Eps: 0.42063508256476556\n",
      "Step 9000, Total reward: -200.0, Eps: 0.4125039631431931\n",
      "Step 9200, Total reward: -200.0, Eps: 0.404533850673669\n",
      "Step 9400, Total reward: -200.0, Eps: 0.3967215570049359\n",
      "Step 9600, Total reward: -200.0, Eps: 0.3890639571153609\n",
      "Step 9800, Total reward: -200.0, Eps: 0.38155798786288553\n",
      "Step 10000, Total reward: -200.0, Eps: 0.3742006467597279\n",
      "Episode 51 of 300\n",
      "Step 10200, Total reward: -200.0, Eps: 0.3669889907713475\n",
      "Step 10400, Total reward: -200.0, Eps: 0.35992013513919235\n",
      "Step 10600, Total reward: -200.0, Eps: 0.3529912522267568\n",
      "Step 10800, Total reward: -200.0, Eps: 0.34619957038848975\n",
      "Step 11000, Total reward: -200.0, Eps: 0.3395423728610988\n",
      "Step 11200, Total reward: -200.0, Eps: 0.33301699667680906\n",
      "Step 11400, Total reward: -200.0, Eps: 0.3266208315981408\n",
      "Step 11600, Total reward: -200.0, Eps: 0.32035131907377923\n",
      "Step 11800, Total reward: -200.0, Eps: 0.3142059512151199\n",
      "Step 12000, Total reward: -140.0, Eps: 0.3081822697930801\n",
      "Episode 61 of 300\n",
      "Step 12200, Total reward: -200.0, Eps: 0.30227786525477407\n",
      "Step 12400, Total reward: -200.0, Eps: 0.29649037575966014\n",
      "Step 12600, Total reward: -200.0, Eps: 0.2908174862347727\n",
      "Step 12800, Total reward: -200.0, Eps: 0.2852569274486622\n",
      "Step 13000, Total reward: -200.0, Eps: 0.27980647510367246\n",
      "Step 13200, Total reward: -200.0, Eps: 0.27446394894619186\n",
      "Step 13400, Total reward: -200.0, Eps: 0.26922721189452276\n",
      "Step 13600, Total reward: 30.0, Eps: 0.26409416918402034\n",
      "Step 13800, Total reward: -200.0, Eps: 0.2590627675291589\n",
      "Step 14000, Total reward: -200.0, Eps: 0.2541309943021904\n",
      "Episode 71 of 300\n",
      "Step 14200, Total reward: -200.0, Eps: 0.24929687672806605\n",
      "Step 14400, Total reward: -200.0, Eps: 0.2445584810953005\n",
      "Step 14600, Total reward: -200.0, Eps: 0.23991391198246126\n",
      "Step 14800, Total reward: -200.0, Eps: 0.2353613114999746\n",
      "Step 15000, Total reward: -200.0, Eps: 0.23089885854694553\n",
      "Step 15200, Total reward: -90.0, Eps: 0.22652476808269262\n",
      "Step 15400, Total reward: -200.0, Eps: 0.22223729041270818\n",
      "Step 15600, Total reward: -200.0, Eps: 0.21803471048875708\n",
      "Step 15800, Total reward: -200.0, Eps: 0.21391534722283462\n",
      "Step 16000, Total reward: -200.0, Eps: 0.20987755281470882\n",
      "Episode 81 of 300\n",
      "Step 16200, Total reward: -90.0, Eps: 0.2059197120927785\n",
      "Step 16400, Total reward: -200.0, Eps: 0.20204024186798297\n",
      "Step 16600, Total reward: 90.0, Eps: 0.1982375903005053\n",
      "Step 16800, Total reward: -200.0, Eps: 0.19451023627901584\n",
      "Step 17000, Total reward: -200.0, Eps: 0.19085668881220727\n",
      "Step 17200, Total reward: -200.0, Eps: 0.1872754864323783\n",
      "Step 17400, Total reward: -200.0, Eps: 0.1837651966108269\n",
      "Step 17600, Total reward: -200.0, Eps: 0.18032441518482004\n",
      "Step 17800, Total reward: -200.0, Eps: 0.17695176579590954\n",
      "Step 18000, Total reward: -200.0, Eps: 0.17364589933937066\n",
      "Episode 91 of 300\n",
      "Step 18200, Total reward: -200.0, Eps: 0.17040549342454195\n",
      "Step 18400, Total reward: -200.0, Eps: 0.16722925184585147\n",
      "Step 18600, Total reward: -200.0, Eps: 0.16411590406431734\n",
      "Step 18800, Total reward: -200.0, Eps: 0.16106420469931504\n",
      "Step 19000, Total reward: -200.0, Eps: 0.1580729330304087\n",
      "Step 19200, Total reward: -200.0, Eps: 0.15514089250904664\n",
      "Step 19400, Total reward: -200.0, Eps: 0.15226691027992587\n",
      "Step 19600, Total reward: -200.0, Eps: 0.14944983671183454\n",
      "Step 19800, Total reward: -200.0, Eps: 0.14668854493778388\n",
      "Step 20000, Total reward: -200.0, Eps: 0.1439819304042466\n",
      "Episode 101 of 300\n",
      "Step 20200, Total reward: -200.0, Eps: 0.1413289104293205\n",
      "Step 20400, Total reward: -200.0, Eps: 0.13872842376964165\n",
      "Step 20600, Total reward: -200.0, Eps: 0.13617943019587256\n",
      "Step 20800, Total reward: -200.0, Eps: 0.13368091007659658\n",
      "Step 21000, Total reward: -200.0, Eps: 0.13123186397045208\n",
      "Step 21200, Total reward: -200.0, Eps: 0.12883131222634217\n",
      "Step 21400, Total reward: -200.0, Eps: 0.1264782945915614\n",
      "Step 21600, Total reward: -200.0, Eps: 0.12417186982768189\n",
      "Step 21800, Total reward: -200.0, Eps: 0.12191111533404535\n",
      "Step 22000, Total reward: -200.0, Eps: 0.11969512677871053\n",
      "Episode 111 of 300\n",
      "Step 22200, Total reward: -200.0, Eps: 0.11752301773670837\n",
      "Step 22400, Total reward: -200.0, Eps: 0.11539391933546027\n",
      "Step 22585, Total reward: 15.0, Eps: 0.11346205665556254\n",
      "Step 22785, Total reward: -70.0, Eps: 0.11141337067137075\n",
      "Step 22985, Total reward: 90.0, Eps: 0.10940525138764381\n",
      "Step 23185, Total reward: -200.0, Eps: 0.10743689552989297\n",
      "Step 23385, Total reward: -200.0, Eps: 0.10550751572952999\n",
      "Step 23585, Total reward: -140.0, Eps: 0.10361634020890936\n",
      "Step 23785, Total reward: 420.0, Eps: 0.1017626124726068\n",
      "Step 23977, Total reward: 128.0, Eps: 0.100017576267881\n",
      "Episode 121 of 300\n",
      "Step 24147, Total reward: 130.0, Eps: 0.0985002116139085\n",
      "Step 24347, Total reward: -10.0, Eps: 0.09674779001132021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24547, Total reward: -200.0, Eps: 0.09503006868138905\n",
      "Step 24747, Total reward: -20.0, Eps: 0.09334636051267987\n",
      "Step 24941, Total reward: -54.0, Eps: 0.09174502430289509\n",
      "Step 25141, Total reward: 110.0, Eps: 0.09012636437112624\n",
      "Step 25341, Total reward: -200.0, Eps: 0.0885397560534716\n",
      "Step 25487, Total reward: -6.0, Eps: 0.08740140579281697\n",
      "Step 25687, Total reward: -200.0, Eps: 0.08586875527019701\n",
      "Step 25887, Total reward: -200.0, Eps: 0.084366453261282\n",
      "Episode 131 of 300\n",
      "Step 26087, Total reward: -200.0, Eps: 0.08289389882523744\n",
      "Step 26287, Total reward: -60.0, Eps: 0.08145050292065457\n",
      "Step 26464, Total reward: -37.0, Eps: 0.0801969556392145\n",
      "Step 26653, Total reward: -39.0, Eps: 0.07888269209027808\n",
      "Step 26853, Total reward: 190.0, Eps: 0.0775187234006883\n",
      "Step 27053, Total reward: -200.0, Eps: 0.07618176310072046\n",
      "Step 27253, Total reward: 330.0, Eps: 0.07487127638842815\n",
      "Step 27453, Total reward: 120.0, Eps: 0.07358673905165311\n",
      "Step 27653, Total reward: -200.0, Eps: 0.07232763725833323\n",
      "Step 27853, Total reward: 20.0, Eps: 0.07109346735096293\n",
      "Episode 141 of 300\n",
      "Step 28053, Total reward: -200.0, Eps: 0.06988373564512343\n",
      "Step 28253, Total reward: -200.0, Eps: 0.06869795823200246\n",
      "Step 28453, Total reward: -200.0, Eps: 0.06753566078482415\n",
      "Step 28653, Total reward: 10.0, Eps: 0.06639637836911214\n",
      "Step 28853, Total reward: -200.0, Eps: 0.0652796552567095\n",
      "Step 29053, Total reward: -140.0, Eps: 0.06418504474348147\n",
      "Step 29253, Total reward: -200.0, Eps: 0.0631121089706277\n",
      "Step 29453, Total reward: -200.0, Eps: 0.0620604187495331\n",
      "Step 29653, Total reward: -200.0, Eps: 0.06102955339008647\n",
      "Step 29853, Total reward: -200.0, Eps: 0.060019100532399\n",
      "Episode 151 of 300\n",
      "Step 30034, Total reward: 219.0, Eps: 0.05912189898101884\n",
      "Step 30234, Total reward: -200.0, Eps: 0.05814922021150312\n",
      "Step 30434, Total reward: -200.0, Eps: 0.05719580177207017\n",
      "Step 30634, Total reward: -200.0, Eps: 0.056261262282631785\n",
      "Step 30797, Total reward: 27.0, Eps: 0.055513316029468164\n",
      "Step 30997, Total reward: 380.0, Eps: 0.05461209198987578\n",
      "Step 31190, Total reward: 227.0, Eps: 0.05375933419731755\n",
      "Step 31390, Total reward: -200.0, Eps: 0.05289284132499759\n",
      "Step 31590, Total reward: -200.0, Eps: 0.0520435061611198\n",
      "Step 31790, Total reward: -200.0, Eps: 0.051210988960294024\n",
      "Episode 161 of 300\n",
      "Step 31990, Total reward: -200.0, Eps: 0.05039495670453954\n",
      "Step 32190, Total reward: 300.0, Eps: 0.04959508297007348\n",
      "Step 32390, Total reward: -200.0, Eps: 0.048811047796736916\n",
      "Step 32590, Total reward: -170.0, Eps: 0.04804253756000659\n",
      "Step 32790, Total reward: 70.0, Eps: 0.047289244845540875\n",
      "Step 32990, Total reward: -200.0, Eps: 0.04655086832620993\n",
      "Step 33190, Total reward: -200.0, Eps: 0.04582711264156089\n",
      "Step 33368, Total reward: 162.0, Eps: 0.04519503224104705\n",
      "Step 33568, Total reward: -200.0, Eps: 0.0444981239096628\n",
      "Step 33768, Total reward: 260.0, Eps: 0.04381501528782353\n",
      "Episode 171 of 300\n",
      "Step 33958, Total reward: 0.0, Eps: 0.04317859513433742\n",
      "Step 34158, Total reward: -200.0, Eps: 0.042521614932859525\n",
      "Step 34358, Total reward: -200.0, Eps: 0.041877643810982065\n",
      "Step 34542, Total reward: 6.0, Eps: 0.04129645846710695\n",
      "Step 34718, Total reward: 14.0, Eps: 0.040750459681376194\n",
      "Step 34901, Total reward: -13.0, Eps: 0.04019284401413465\n",
      "Step 35081, Total reward: -10.0, Eps: 0.03965423484675562\n",
      "Step 35281, Total reward: -200.0, Eps: 0.03906704165471681\n",
      "Step 35454, Total reward: -53.0, Eps: 0.03856850659616447\n",
      "Step 35649, Total reward: 45.0, Eps: 0.03801681717098266\n",
      "Episode 181 of 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'MountainCar-v0'\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    num_states = env.env.observation_space.shape[0]\n",
    "    print(num_states)\n",
    "    num_actions = env.env.action_space.n\n",
    "    print(num_actions)\n",
    "\n",
    "    model = Model(num_states, num_actions, BATCH_SIZE)\n",
    "    mem = Memory(50000)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(model.var_init)\n",
    "        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,\n",
    "                        LAMBDA)\n",
    "        num_episodes = 300\n",
    "        cnt = 0\n",
    "        while cnt < num_episodes:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
    "            gr.run()\n",
    "            cnt += 1\n",
    "        plt.plot(gr.reward_store)\n",
    "        plt.show()\n",
    "        plt.close(\"all\")\n",
    "        plt.plot(gr.max_x_store)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
