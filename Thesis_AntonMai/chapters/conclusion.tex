\chapter{Conclusion and Future Work}

Using RL for robotic arms has been difficult due to sparse rewards in robotic arm tasks. The concept of HER enabled working with sparse rewards by making the training samples more efficient through hindsight replay. Most of the research on HER focuses on improving the performance of HER. Instead in this thesis, harder tasks were constructed to use HER on. 
\newline
Two environments were built to test HER on, FetchSlideball and FetchToss. 
\newline
FetchSlideball is similar to the already existing task FetchSlide which is usually used as a benchmark for HER. The task involves using the robotic arm to slide an object to a point that might be outside of the robotic arms' reach. The only difference between both environments is that FetchSlideball uses a ball instead a cylinder and the distance to the goal was increased. We first tested the FetchSlide task with a ball and compared that to the cylinder used in the default FetchSlide task. The results showed that using a ball improved the success rate for this task by 20\% (from about 60\% to about 80\%). In a first experiment of the FetchSlideball task with the increased distance, the success rate was null. This was due to the ball not being able to roll far enough and therefore the task was impossible. To be able to reach the goal, the friction of the ball had to be modified. After halving the balls' friction the experiment was repeated. This time it was barely possible to reach the goal and solve the task, but the agent still showed almost no success. Reason for that is probably as Ren et al. stated \cite{hgg}, the goals that were learned through HER were not useful enough in learning how to solve the actual goal. Changing the balls friction to 10\% of its original friction showed interesting results. After 50 epochs, the success rate was at 8\%, which is an improvement to the former experiments. The agent learned to roll the ball in the right direction. However, instead of controlling the ball to stop at the right spot, the agent exploited the fact that a training episode only takes limited time and an episode is successful if the ball is in the goal space after the last time step. By rolling the ball with the right timing, the agent could get successful episodes by having the ball at the goal space when the episode ends - even though the ball would roll farther if the episode did not end. This showed that the agent at least learned how to roll the ball in the right direction, even for greater goal distances. 
\newline
FetchToss is a much harder environment. The agent is required to use the robotic arm to fetch a ball and toss it into a box that is slightly out of the robotic arms' range. First we compared fetching a ball in contrast to fetching a cube by comparing the FetchPickAndPlace task with a ball and cube. Fetching a ball proved to be about 5\% worse than fetching a cube (from about 95\% to about 90\%). When trying to train the agent to solve the FetchToss task, results showed no success at all. Even changing the box to make it easier to toss into, doubling the time steps for each episode, making the ball lighter or using a cube did not show any success. A path to solve the task was coded by hand which proved that it is possible to solve this task. It was just too hard for HER to solve. FetchToss requires many intermediate steps like finding the ball, picking it up, moving it in the right direction and releasing it with the right timing to toss it. Having all these steps increases the difficulty greatly. Using the energy-based experience prioritization approach by Zhao and Tresp \cite{energyher} in the future might help guide the agent to learn tossing because tossing requires a high amount of energy. 
\newline
This thesis showed that vanilla HER fails for both tasks FetchSlideball and FetchToss. For tasks with a high goal distance and very complicated solutions HER seemed to struggle.
Future work needs to focus on solving these tasks. An obvious approach would be to use improved HER algorithms like energy-based hindsight experience prioritization to solve these tasks. Also modifying these tasks might be a possibility for research. Finding out how much different goal distances affect the success rate might be interesting. After solutions are found to make these tasks work, these tasks could be extended to a golf and basketball environment as it was intended from the start. This could include using different obstacles like walls for a golf environment. Also, other objects could be used. It would be very interesting to research tossing paper planes.   
 
 
 
 